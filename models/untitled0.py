# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EbkLlLx8c4ujO7kkpuQdzkZ64iD0nE7E
"""

from google.colab import files
uploaded = files.upload()

import zipfile
import pandas as pd

# Extract the zip file
with zipfile.ZipFile("urlset.csv.zip", 'r') as zip_ref:
    zip_ref.extractall()

# Load CSV with encoding and skip bad lines
df = pd.read_csv("urlset.csv", encoding='ISO-8859-1', on_bad_lines='skip')

# Show first 5 rows
df.head()

# Basic info about dataset
df.info()

# Check if there are missing values
df.isnull().sum()

# Check unique values in the target column
print(df['label'].unique())

# X = All columns except the label
X = df.drop('label', axis=1)

# y = Only the label
y = df['label']

from sklearn.model_selection import train_test_split

# Split data: 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# See data types of each column
print(X.dtypes)

# Drop all non-numeric (object) columns
X = X.drop(['domain', 'ranking', 'mld_res', 'mld.ps_res', 'jaccard_ARrd', 'jaccard_ARrem'], axis=1)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Drop rows where 'label' is missing
df = df.dropna(subset=['label'])

# Recreate X and y after dropping NaNs
X = df.drop(['label', 'domain', 'ranking', 'mld_res', 'mld.ps_res', 'jaccard_ARrd', 'jaccard_ARrem'], axis=1)
y = df['label']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

data = pd.read_csv('urlset.csv', encoding='latin1', on_bad_lines='skip')
print(data.head())
print(f"Data loaded with shape: {data.shape}")

data = pd.read_csv('urlset.csv', encoding='latin1', on_bad_lines='skip')
print(data.shape)
print(data.head())

with open('urlset.csv', encoding='latin1') as f:
    lines = f.readlines()
    for i in range(18230, 18265):
        print(f"Line {i+1}: {lines[i]}")

import csv

data = pd.read_csv('urlset.csv', encoding='latin1', quoting=csv.QUOTE_ALL, on_bad_lines='skip')

data = pd.read_csv('urlset.csv', encoding='latin1', quotechar='"', on_bad_lines='skip')

print(data.info())        # Check data types, missing values
print(data.describe())    # Summary stats for numerical columns
print(data.head())        # See first few rows
print(data.columns)       # Check column names

# Suppose 'label' is the target column with 0 = legitimate, 1 = phishing
X = data.drop('label', axis=1)
y = data['label']

# If any categorical columns in X, encode them using one-hot or label encoding
# For example:
# from sklearn.preprocessing import LabelEncoder
# le = LabelEncoder()
# X['category_col'] = le.fit_transform(X['category_col'])

# Or for many categorical columns:
# X = pd.get_dummies(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

print(data.columns)

# Drop rows where 'label' is NaN
data_clean = data.dropna(subset=['label'])

# Now re-extract features and target
urls = data_clean['domain']
labels = data_clean['label']

# Then do TF-IDF and train model as before
tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_features=1000)
X = tfidf.fit_transform(urls)

X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.2, random_state=42
)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

model = RandomForestClassifier(
    n_estimators=100, max_depth=None, random_state=42
)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import randint

# Define parameter distribution
param_dist = {
    'n_estimators': randint(100, 300),
    'max_depth': [10, 20, None],
    'max_features': ['sqrt'],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=10,
    cv=3,
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the model
random_search.fit(X_train, y_train)

# Save the best model
best_model = random_search.best_estimator_

y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

print("Training Accuracy:", accuracy_score(y_train, y_train_pred))
print("Test Accuracy:", accuracy_score(y_test, y_test_pred))

from sklearn.ensemble import RandomForestClassifier

tuned_model = RandomForestClassifier(
    n_estimators=200,          # More trees help, but tune for speed
    max_depth=20,              # Prevents trees from growing too deep
    min_samples_split=5,       # A node must have at least 5 samples to split
    min_samples_leaf=2,        # Leaves must have at least 2 samples
    max_features='sqrt',       # Limits the number of features considered at each split
    random_state=42,
    n_jobs=-1
)

# Train again
tuned_model.fit(X_train, y_train)

# Accuracy scores
print("Training Accuracy:", tuned_model.score(X_train, y_train))
print("Test Accuracy:", tuned_model.score(X_test, y_test))

# Detailed report
from sklearn.metrics import classification_report
y_pred = tuned_model.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(tuned_model, X, labels, cv=5, scoring='accuracy')
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())

import pickle

# Save the model
with open('phishing_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)

from google.colab import files

# Download the file
files.download('phishing_model.pkl')